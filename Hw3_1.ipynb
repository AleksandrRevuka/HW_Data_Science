{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEyv9gVsrEuCqMCIMkzmfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleksandrRevuka/HW_Data_Science/blob/main/Hw3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "напишіть функцію гіпотези лінійної регресії у векторному вигляді;"
      ],
      "metadata": {
        "id": "jBqt9Ae46Agk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMQDbspi505N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def hypothesis(theta, X):\n",
        "    return np.dot(X, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "створіть функцію для обчислення функції втрат у векторному вигляді;"
      ],
      "metadata": {
        "id": "_-aEEXMK6K_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    J = (1 / (2 * m)) * np.sum(np.square(X.dot(theta) - y))\n",
        "    return J"
      ],
      "metadata": {
        "id": "Ziff6dsX6PGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "реалізуйте один крок градієнтного спуску;"
      ],
      "metadata": {
        "id": "9_8ECXtP6ZwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_step(X, y, theta, learning_rate):\n",
        "\n",
        "    m = len(y)\n",
        "    predictions = np.dot(X, theta)\n",
        "    errors = predictions - y\n",
        "    gradient = (1 / m) * np.dot(X.T, errors)\n",
        "    theta = theta - learning_rate * gradient\n",
        "    return theta"
      ],
      "metadata": {
        "id": "MDbo8icJ6eUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}